{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470f47dc",
   "metadata": {},
   "source": [
    "REINFORCE is one for policy based RL Algorithm. Compared with value based functions of RL there are some critical advantages. Please refer chapter 13 of the book \"Reinforcement Learning An introduction second edition\" by Richard from more details.\n",
    "\n",
    "In this notebook we implement REINFORCE Algorithm with Artificial neural network. By leveraging Deep Learing, the REINFORCE has better performence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354b7ce",
   "metadata": {},
   "source": [
    "# How REINFORCE Works\n",
    "\n",
    "1. Collect Episodes\n",
    "1. Calculate Returns\n",
    "1. Policy Gradient Update\n",
    "1. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "#env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "act_space = env.action_space.n\n",
    "\n",
    "print(f\"obs_space is ${obs_space}\")\n",
    "print(f\"act_space is ${act_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c0b80",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e4dc2",
   "metadata": {},
   "source": [
    "Using an artificial neural network to present parametered policy. In our example, we use network which has a signle hiden layer with 512 neural unites and full connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f22725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, device, action_space, dim_in = 128):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(dim_in, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space)\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        size = len(dataloader.dataset)\n",
    "        nn.Module().train()\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            pred = self(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f} [{current:>5f}/{size:>5f}]\")\n",
    "\n",
    "    def test(self, dataloader):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        nn.Module.eval(nn.Module())\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self(X)\n",
    "                test_loss += self.loss_fun(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    def save(self, file = \"model.pth\"):\n",
    "        torch.save(self.state_dict(), file)\n",
    "        print(f\"saved the model in {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3070bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e15943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReplyBuffer(Dataset):\n",
    "    def __init__(self, maxsize: int = 128):\n",
    "        self.buffer = []\n",
    "        self.maxsize =min(56, maxsize)\n",
    "        self.index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.buffer[idx]\n",
    "    \n",
    "    def record(self, item):\n",
    "        next = (self.index + 1) / self.maxsize\n",
    "        self.buffer[next] = item\n",
    "        self.index = next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_returns(rewards, gamma):\n",
    "    returns = np.zeros_like(rewards, dtype=np.float32)\n",
    "    running_return = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_return = rewards[t] + gamma * running_return\n",
    "        returns[t] = running_return\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create modul\n",
    "policy = PolicyNetwork(device, action_space=act_space, dim_in= 128)\n",
    "print(policy)\n",
    "\n",
    "# Create data loaders.\n",
    "training_data = ReplyBuffer(512)\n",
    "train_dataloader = DataLoader(training_data, batch_size=56)\n",
    "\n",
    "num_episodes = 200\n",
    "gamma = 9e-1\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    while not done:\n",
    "        state_input = np.array(state, dtype=np.float32).reshape(1, -1)\n",
    "        probs = policy.forward(state_input).numpy()[0]\n",
    "        action = np.random.choice(act_space, p=probs)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        states.append(state_input[0])\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "    # compute returns of env\n",
    "    r = compute_returns(rewards, gamma)\n",
    "    training_data.record(np.vstack(states), actions, r)\n",
    "\n",
    "    policy.train(training_data)\n",
    "# Train model in given dataset\n",
    "\n",
    "# module.train(train_dataloader, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_RL_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
