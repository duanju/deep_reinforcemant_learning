{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25c1300",
   "metadata": {},
   "source": [
    "# Getting start\n",
    "\n",
    "## Introduction for game \n",
    "\n",
    "**Observation Space**\n",
    "The state is an 8-dimensional vector: \n",
    "\n",
    "* 1-2 the coordinates of the lander in x & y, \n",
    "* 3-4 its linear velocities in x & y, its angle, \n",
    "* 5-6 its angular velocity\n",
    "* 7-8 two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "**Action Space**\n",
    "There are four discrete actions available:\n",
    "\n",
    "```\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "```\n",
    "\n",
    "**Rewards**\n",
    "* is increased/decreased the closer/further the lander is to the landing pad.\n",
    "* is increased/decreased the slower/faster the lander is moving.\n",
    "* is decreased the more the lander is tilted (angle not horizontal).\n",
    "* is increased by 10 points for each leg that is in contact with the ground.\n",
    "* is decreased by 0.03 points each frame a side engine is firing.\n",
    "* is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.<p/>\n",
    "An episode is considered a solution if it scores at least 200 points.\n",
    "\n",
    "## Create [gym env](https://gymnasium.farama.org/introduction/basic_usage/)\n",
    "\n",
    "1. Create conda env from environment.yml\n",
    "\n",
    "```\n",
    "    mamba env create -f environment.yml\n",
    "```\n",
    "\n",
    "2. Add `gymnasium` to environment\n",
    "\n",
    "```\n",
    "    mamba install -c conda-forge gym==1.0.0\n",
    "    mamba install conda-forge::pygame\n",
    "````\n",
    "\n",
    "3. Import gymnasium into project\n",
    "\n",
    "4. Create gym env 'LunarLander-v3'\n",
    "\n",
    "> This environment is part of the Box2D environments which contains general information about the environment.\n",
    "> Action Space: Discrete(4)\n",
    "> Observation Space:\n",
    "> Box([ -2.5 -2.5 -10. -10. -6.2831855 -10. -0. -0. ], [ 2.5 2.5 10. 10. 6.2831855 10. 1. 1. ], (8,), float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# print(state_size, action_size)\n",
    "# 8 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5db65",
   "metadata": {},
   "source": [
    "## Add deep learning dependencies\n",
    "\n",
    "1. install tensorflow & keras\n",
    "\n",
    "```\n",
    "    mamba install conda-forge::tensorflow\n",
    "    mamba install conda-forge::keras\n",
    "\n",
    "````\n",
    "\n",
    "2. import tensorflow packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41952869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9352f",
   "metadata": {},
   "source": [
    "## Add matplotlib\n",
    "\n",
    "```\n",
    "    mamba install conda-forge::matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30db4c9",
   "metadata": {},
   "source": [
    "## Implement DQN Algorithm\n",
    "\n",
    "1. Define experience replay buff\n",
    "2. Create both current and target networks\n",
    "3. Play and get feedbacks from Env\n",
    "4. Collect the feedbacks and cache them in buff\n",
    "5. Tain the target network by simpling data from buff\n",
    "6. Update current network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "MEMORY_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "EPISODES = 20\n",
    "\n",
    "\n",
    "def preprocess_state(state):\n",
    "    return np.reshape(state, [1, state_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4efba8",
   "metadata": {},
   "source": [
    "DQNAgent class contains all network implementations and expose interfaces for updating target network\n",
    "\n",
    "Topology of Network\n",
    "\n",
    "```\n",
    "+-------------------+       +-------------------+       +-------------------+       +-------------------+\n",
    "|    Input Layer    |       |   Hidden Layer 1  |       |   Hidden Layer 2  |       |    Output Layer   |\n",
    "|                   |       |                   |       |                   |       |                   |\n",
    "|   8-dimensional   |------>|    64-dimensional  |------>|    64-dimensional  |------>|    4-dimensional   |\n",
    "|   (State Space)   |  W₁   |     (ReLU)        |  W₂   |     (ReLU)        |  W₃   |    (Linear)       |\n",
    "+-------------------+       +-------------------+       +-------------------+       +-------------------+\n",
    "      ↑\n",
    "      | (8 values)\n",
    "      | [x,y,x_vel,y_vel,angle,angular_vel,left_leg,right_leg]\n",
    "      |\n",
    "Environment\n",
    "```\n",
    "\n",
    "Full Parameter Count:\n",
    "\n",
    "```\n",
    "┌──────────────┬───────────────────┐\n",
    "│ Layer        │ Parameters        │\n",
    "├──────────────┼───────────────────┤\n",
    "│ Input→Hidden1│ 576               │\n",
    "│ Hidden1→2    │ 4,160             │\n",
    "│ Hidden2→Out  │ 260               │\n",
    "├──────────────┼───────────────────┤\n",
    "│ TOTAL        │ **4,996**         │\n",
    "└──────────────┴───────────────────┘\n",
    "```\n",
    "\n",
    "Activation Flow:\n",
    "\n",
    "```\n",
    "State → ReLU(ReLU(State·W₁ + b₁)·W₂ + b₂)·W₃ + b₃ → Q-values\n",
    "```\n",
    "\n",
    "Output Interpretation:\n",
    "\n",
    "```\n",
    "[Do nothing, Fire left, Fire main, Fire right]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3999ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.expirence_replay_buff = deque(maxlen=MEMORY_SIZE)\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        \n",
    "        # Main(Current) network\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        # Target network\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            Dense(64, input_dim=self.state_size, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.expirence_replay_buff.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.randint(self.action_size)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.expirence_replay_buff) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        minibatch = np.random.choice(len(self.expirence_replay_buff), BATCH_SIZE, replace=False)\n",
    "        states = np.zeros((BATCH_SIZE, self.state_size))\n",
    "        next_states = np.zeros((BATCH_SIZE, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "        \n",
    "        for i, idx in enumerate(minibatch):\n",
    "            state, action, reward, next_state, done = self.expirence_replay_buff[idx]\n",
    "            states[i] = state\n",
    "            next_states[i] = next_state\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        # Target Q values\n",
    "        target_q = self.target_model.predict(next_states, verbose=0)\n",
    "        max_target_q = np.amax(target_q, axis=1)\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones[i]:\n",
    "                current_q[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                current_q[i][actions[i]] = rewards[i] + GAMMA * max_target_q[i]\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(states, current_q, verbose=0)\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, \n",
    "                                  self.exploration_rate * EXPLORATION_DECAY)\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_rewards = np.empty(EPISODES)  # List of rewards per episode\n",
    "exploration_rates = np.empty(EPISODES)  # List of exploration rates per episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    init_state, _ = env.reset()\n",
    "    state = preprocess_state(init_state)\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    \n",
    "    env_start = time.time()\n",
    "    while True:\n",
    "        step += 1\n",
    "        env.render()\n",
    "\n",
    "        now = time.time()\n",
    "        t = now - env_start\n",
    "        env_start = now \n",
    "        # print(f\"1 - {t:.4f} seconds\")\n",
    "\n",
    "        action = agent.act(state)\n",
    "\n",
    "        now = time.time()\n",
    "        t = now - env_start\n",
    "        env_start = now \n",
    "        # print(f\"2 - {t:.4f} seconds\")\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        now = time.time()\n",
    "        t = now - env_start\n",
    "        env_start = now \n",
    "        # print(f\"3 - {t:.4f} seconds\")\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        next_state = preprocess_state(next_state)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        now = time.time()\n",
    "        t = now - env_start\n",
    "        env_start = now \n",
    "        # print(f\"4 - {t:.4f} seconds\")\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.replay()\n",
    "\n",
    "\n",
    "        now = time.time()\n",
    "        t = now - env_start\n",
    "        env_start = now \n",
    "        # print(f\"5 - {t:.4f} seconds\")\n",
    "\n",
    "        if step % TARGET_UPDATE_FREQ == 0:\n",
    "\n",
    "\n",
    "            now = time.time()\n",
    "            t = now - env_start\n",
    "            env_start = now \n",
    "            # print(f\"6 - {t:.4f} seconds\")\n",
    "\n",
    "            agent.update_target()\n",
    "\n",
    "            now = time.time()\n",
    "            t = now - env_start\n",
    "            env_start = now \n",
    "            # print(f\"7 - {t:.4f} seconds\")\n",
    "        \n",
    "        if done:\n",
    "            total_rewards[e] = total_reward\n",
    "            exploration_rates[e] = agent.exploration_rate\n",
    "            print(f\"Episode: {e+1}/{EPISODES}, Score: {total_reward:.2f}, Exploration: {agent.exploration_rate:.2f}\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e19e7",
   "metadata": {},
   "source": [
    "## Show the learning performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = np.arange(1, EPISODES+1)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot Total Reward\n",
    "ax1.plot(episodes, total_rewards, 'b-', linewidth=1)\n",
    "ax1.set_title('Training Performance')\n",
    "ax1.set_ylabel('Total Reward', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Exploration Rate\n",
    "ax2.plot(episodes, exploration_rates, 'r-', linewidth=1)\n",
    "ax2.set_xlabel('Episodes')\n",
    "ax2.set_ylabel('Exploration Rate', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add horizontal line at solved threshold (LunarLander: 200)\n",
    "ax1.axhline(y=200, color='g', linestyle='--', label='Solved Threshold')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_RL_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
